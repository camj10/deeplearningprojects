{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ed95c4",
   "metadata": {},
   "source": [
    "# Visión con Transformers + OpenCV\n",
    "Sistema de cámara en vivo con detección de rostro (Haar Cascade) y modelos HF para **emociones** y **edad**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f569e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuración de la carpeta raíz al path\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a1350a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelos (puede demorar la 1ra vez si hay descarga/caché)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Este cuaderno requiere webcam disponible.\n",
    "# Presiona 'q' para salir de la ventana.\n",
    "import os, cv2, numpy as np, tempfile, torch\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "print(\"Cargando modelos (puede demorar la 1ra vez si hay descarga/caché)...\")\n",
    "processor_emotion = AutoImageProcessor.from_pretrained(\"dima806/facial_emotions_image_detection\")\n",
    "model_emotion = AutoModelForImageClassification.from_pretrained(\"dima806/facial_emotions_image_detection\")\n",
    "processor_age = AutoImageProcessor.from_pretrained(\"Robys01/facial_age_estimator\")\n",
    "model_age = AutoModelForImageClassification.from_pretrained(\"Robys01/facial_age_estimator\")\n",
    "\n",
    "emotion_labels = {\n",
    "    \"sad\": \"Triste\",\"disgust\": \"Disgusto\",\"angry\": \"Enojado\",\n",
    "    \"neutral\": \"Neutral\",\"fear\": \"Miedo\",\"surprise\": \"Sorpresa\",\"happy\": \"Feliz\"\n",
    "}\n",
    "age_labels = {0:'01',1:'02',2:'03',3:'04-05',4:'06-07',5:'08-09',6:'10-12',7:'13-24',8:'26-30',9:'26-30',\n",
    "              10:'26-30',11:'31-35',12:'36-40',13:'41-45',14:'46-50',15:'51-55',16:'56-60',17:'61-70',\n",
    "              18:'71-80',19:'81-90',20:'90+'}\n",
    "\n",
    "# Haar cascade\n",
    "src_path = os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml')\n",
    "dest_path = os.path.join(tempfile.gettempdir(), 'haarcascade_frontalface_default.xml')\n",
    "if not os.path.exists(dest_path):\n",
    "    with open(src_path,'rb') as s, open(dest_path,'wb') as d: d.write(s.read())\n",
    "face_cascade = cv2.CascadeClassifier(dest_path)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        face_img = frame[y:y+h, x:x+w]\n",
    "        face_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            em_inputs = processor_emotion(images=face_pil, return_tensors=\"pt\")\n",
    "            em_logits = model_emotion(**em_inputs).logits\n",
    "            em_id = int(em_logits.argmax(-1).item())\n",
    "            em_key = list(emotion_labels.keys())[em_id]\n",
    "            emotion_text = emotion_labels[em_key]\n",
    "\n",
    "            ag_inputs = processor_age(images=face_pil, return_tensors=\"pt\")\n",
    "            ag_logits = model_age(**ag_inputs).logits\n",
    "            ag_id = int(ag_logits.argmax(-1).item())\n",
    "            age_text = age_labels.get(ag_id, '?')\n",
    "\n",
    "        cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)\n",
    "        cv2.putText(frame, f'Emocion: {emotion_text}', (x, y-30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "        cv2.putText(frame, f'Edad: {age_text}', (x, y-8),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Emociones y Edad (q para salir)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release(); cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
